"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[36145],{15680:(e,t,n)=>{n.d(t,{xA:()=>m,yg:()=>g});var a=n(296540);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),d=s(n),c=r,g=d["".concat(p,".").concat(c)]||d[c]||u[c]||o;return n?a.createElement(g,i(i({ref:t},m),{},{components:n})):a.createElement(g,i({ref:t},m))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=c;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[d]="string"==typeof e?e:r,i[1]=l;for(var s=2;s<o;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},4854:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var a=n(58168),r=(n(296540),n(15680));const o={title:"Export",language:"en"},i=void 0,l={unversionedId:"data-operate/export/export-manual",id:"data-operate/export/export-manual",title:"Export",description:"\x3c!--",source:"@site/docs/data-operate/export/export-manual.md",sourceDirName:"data-operate/export",slug:"/data-operate/export/export-manual",permalink:"/docs/dev/data-operate/export/export-manual",draft:!1,tags:[],version:"current",frontMatter:{title:"Export",language:"en"},sidebar:"docs",previous:{title:"Export Overview",permalink:"/docs/dev/data-operate/export/export-overview"},next:{title:"SELECT INTO OUTFILE",permalink:"/docs/dev/data-operate/export/outfile"}},p={},s=[{value:"Overview",id:"overview",level:2},{value:"Export File Column Type Mapping",id:"export-file-column-type-mapping",level:2},{value:"Examples",id:"examples",level:2},{value:"Export to HDFS",id:"export-to-hdfs",level:3},{value:"Export to S3",id:"export-to-s3",level:3},{value:"Export to Local File System",id:"export-to-local-file-system",level:3},{value:"Exporting Specific Partitions",id:"exporting-specific-partitions",level:3},{value:"Filtering Data During Export",id:"filtering-data-during-export",level:3},{value:"Exporting External Table Data",id:"exporting-external-table-data",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Consistent Export",id:"consistent-export",level:3},{value:"Export Job Concurrency",id:"export-job-concurrency",level:3},{value:"Clear Export Directory Before Exporting",id:"clear-export-directory-before-exporting",level:3},{value:"Set Export File Size",id:"set-export-file-size",level:3},{value:"Notes",id:"notes",level:2},{value:"Appendix",id:"appendix",level:2},{value:"Principles of Concurrent Export",id:"principles-of-concurrent-export",level:3},{value:"Example",id:"example",level:3},{value:"Optimizing Export Performance",id:"optimizing-export-performance",level:3}],m={toc:s},d="wrapper";function u(e){let{components:t,...n}=e;return(0,r.yg)(d,(0,a.A)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("p",null,"This document provides an overview of using the ",(0,r.yg)("inlineCode",{parentName:"p"},"EXPORT")," command to export data stored in Doris."),(0,r.yg)("p",null,"For detailed information on the ",(0,r.yg)("inlineCode",{parentName:"p"},"EXPORT")," command, please refer to: ",(0,r.yg)("a",{parentName:"p",href:"/docs/dev/sql-manual/sql-statements/Data-Manipulation-Statements/Manipulation/EXPORT"},"EXPORT"),"."),(0,r.yg)("h2",{id:"overview"},"Overview"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"Export")," is a feature provided by Doris to asynchronously export data. This functionality allows users to export data from specified tables or partitions in a specified file format to a target storage system, including object storage, HDFS, or local file systems."),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"Export")," is an asynchronous command that returns immediately after execution. Users can view detailed information about the export task using the ",(0,r.yg)("inlineCode",{parentName:"p"},"Show Export")," command."),(0,r.yg)("p",null,"For guidance on choosing between ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"EXPORT"),", please refer to the ",(0,r.yg)("a",{parentName:"p",href:"/docs/dev/data-operate/export/export-overview"},"Export Overview"),"."),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"EXPORT")," command currently supports exporting the following types of tables or views:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Doris internal tables"),(0,r.yg)("li",{parentName:"ul"},"Doris logical views"),(0,r.yg)("li",{parentName:"ul"},"Doris Catalog tables")),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"EXPORT")," command currently supports the following export formats:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Parquet"),(0,r.yg)("li",{parentName:"ul"},"ORC"),(0,r.yg)("li",{parentName:"ul"},"CSV"),(0,r.yg)("li",{parentName:"ul"},"CSV_with_names"),(0,r.yg)("li",{parentName:"ul"},"CSV_with_names_and_types")),(0,r.yg)("p",null,"Compression formats are not supported."),(0,r.yg)("p",null,"Example:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'mysql> EXPORT TABLE tpch1.lineitem TO "s3://my_bucket/path/to/exp_"\n    -> PROPERTIES(\n    ->     "format" = "csv",\n    ->     "max_file_size" = "2048MB"\n    -> )\n    -> WITH s3 (\n    ->   "s3.endpoint" = "${endpoint}",\n    ->   "s3.region" = "${region}",\n    ->   "s3.secret_key"="${sk}",\n    ->   "s3.access_key" = "${ak}"\n    -> );\n')),(0,r.yg)("p",null,"After submitting the job, you can query the export job status using the ",(0,r.yg)("a",{parentName:"p",href:"/docs/dev/sql-manual/sql-statements/Show-Statements/SHOW-EXPORT"},"SHOW EXPORT")," command. An example result is shown below:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'mysql> show export\\G\n*************************** 1. row ***************************\n      JobId: 143265\n      Label: export_0aa6c944-5a09-4d0b-80e1-cb09ea223f65\n      State: FINISHED\n   Progress: 100%\n   TaskInfo: {"partitions":[],"parallelism":5,"data_consistency":"partition","format":"csv","broker":"S3","column_separator":"\\t","line_delimiter":"\\n","max_file_size":"2048MB","delete_existing_files":"","with_bom":"false","db":"tpch1","tbl":"lineitem"}\n       Path: s3://ftw-datalake-test-1308700295/test_ycs_activeDefense_v10/test_csv/exp_\n CreateTime: 2024-06-11 18:01:18\n  StartTime: 2024-06-11 18:01:18\n FinishTime: 2024-06-11 18:01:31\n    Timeout: 7200\n   ErrorMsg: NULL\nOutfileInfo: [\n  [\n    {\n      "fileNumber": "1",\n      "totalRows": "6001215",\n      "fileSize": "747503989bytes",\n      "url": "s3://my_bucket/path/to/exp_6555cd33e7447c1-baa9568b5c4eb0ac_*"\n    }\n  ]\n]\n1 row in set (0.00 sec)\n')),(0,r.yg)("p",null,"The columns in the ",(0,r.yg)("inlineCode",{parentName:"p"},"show export")," command result have the following meanings:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"JobId: The unique ID of the job"),(0,r.yg)("li",{parentName:"ul"},"Label: The label of the export job. If not specified, the system generates one by default."),(0,r.yg)("li",{parentName:"ul"},"State: Job status:",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"PENDING: Job is pending scheduling"),(0,r.yg)("li",{parentName:"ul"},"EXPORTING: Data is being exported"),(0,r.yg)("li",{parentName:"ul"},"FINISHED: Job completed successfully"),(0,r.yg)("li",{parentName:"ul"},"CANCELLED: Job failed"))),(0,r.yg)("li",{parentName:"ul"},"Progress: Job progress. This is measured by the number of query plans. If there are 10 threads in total and 3 have completed, the progress is 30%."),(0,r.yg)("li",{parentName:"ul"},"TaskInfo: Job information displayed in JSON format:",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"db: Database name"),(0,r.yg)("li",{parentName:"ul"},"tbl: Table name"),(0,r.yg)("li",{parentName:"ul"},"partitions: Specified partitions to export. An empty list indicates all partitions."),(0,r.yg)("li",{parentName:"ul"},"column_separator: Column separator for the export file."),(0,r.yg)("li",{parentName:"ul"},"line_delimiter: Line separator for the export file."),(0,r.yg)("li",{parentName:"ul"},"tablet num: Total number of involved tablets."),(0,r.yg)("li",{parentName:"ul"},"broker: Name of the broker used."),(0,r.yg)("li",{parentName:"ul"},"coord num: Number of query plans."),(0,r.yg)("li",{parentName:"ul"},"max_file_size: Maximum size of an export file."),(0,r.yg)("li",{parentName:"ul"},"delete_existing_files: Whether to delete existing files and directories in the export directory."),(0,r.yg)("li",{parentName:"ul"},"columns: Columns to export; an empty value means all columns are exported."),(0,r.yg)("li",{parentName:"ul"},"format: File format of the export."))),(0,r.yg)("li",{parentName:"ul"},"Path: Export path in the remote storage."),(0,r.yg)("li",{parentName:"ul"},"CreateTime/StartTime/FinishTime: Job creation time, scheduling start time, and end time."),(0,r.yg)("li",{parentName:"ul"},"Timeout: Job timeout period in seconds, starting from CreateTime."),(0,r.yg)("li",{parentName:"ul"},"ErrorMsg: If there is an error in the job, the reason is displayed here."),(0,r.yg)("li",{parentName:"ul"},"OutfileInfo: If the job is successful, this displays detailed ",(0,r.yg)("inlineCode",{parentName:"li"},"SELECT INTO OUTFILE")," result information.")),(0,r.yg)("p",null,"After submitting an export job, you can cancel it before it succeeds or fails using the ",(0,r.yg)("a",{parentName:"p",href:"/docs/dev/sql-manual/sql-statements/Data-Manipulation-Statements/Manipulation/CANCEL-EXPORT"},"CANCEL EXPORT")," command. An example cancellation command is shown below:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'CANCEL EXPORT FROM tpch1 WHERE LABEL like "%export_%";\n')),(0,r.yg)("h2",{id:"export-file-column-type-mapping"},"Export File Column Type Mapping"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"Export")," supports exporting data in Parquet and ORC file formats. These formats have their own data types. Doris's export functionality automatically maps Doris's data types to the corresponding data types in Parquet and ORC file formats. For detailed mapping relationships, please refer to the \"Export File Column Type Mapping\" section in the ",(0,r.yg)("a",{parentName:"p",href:"/docs/dev/data-operate/export/export-overview"},"Export Overview")," document."),(0,r.yg)("h2",{id:"examples"},"Examples"),(0,r.yg)("h3",{id:"export-to-hdfs"},"Export to HDFS"),(0,r.yg)("p",null,"Export the ",(0,r.yg)("inlineCode",{parentName:"p"},"col1")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"col2")," columns from partitions ",(0,r.yg)("inlineCode",{parentName:"p"},"p1")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"p2")," of the ",(0,r.yg)("inlineCode",{parentName:"p"},"db1.tbl1")," table to HDFS, setting the export job label to ",(0,r.yg)("inlineCode",{parentName:"p"},"mylabel"),". The export file format is CSV (default format), the column separator is ",(0,r.yg)("inlineCode",{parentName:"p"},","),", and the maximum file size is 512MB."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE db1.tbl1 \nPARTITION (p1, p2)\nTO "hdfs://host/path/to/export/" \nPROPERTIES\n(\n    "label" = "mylabel",\n    "column_separator" = ",",\n    "max_file_size" = "512MB",\n    "columns" = "col1, col2"\n)\nWITH HDFS (\n    "fs.defaultFS" = "hdfs://hdfs_host:port",\n    "hadoop.username" = "hadoop"\n);\n')),(0,r.yg)("p",null,"If HDFS high availability is enabled, provide HA information as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE db1.tbl1 \nPARTITION (p1, p2)\nTO "hdfs://HDFS8000871/path/to/export/" \nPROPERTIES\n(\n    "label" = "mylabel",\n    "column_separator" = ",",\n    "max_file_size" = "512MB",\n    "columns" = "col1, col2"\n)\nWITH HDFS (\n    "fs.defaultFS" = "hdfs://HDFS8000871",\n    "hadoop.username" = "hadoop",\n    "dfs.nameservices" = "your-nameservices",\n    "dfs.ha.namenodes.your-nameservices" = "nn1, nn2",\n    "dfs.namenode.rpc-address.HDFS8000871.nn1" = "ip:port",\n    "dfs.namenode.rpc-address.HDFS8000871.nn2" = "ip:port",\n    "dfs.client.failover.proxy.provider.HDFS8000871" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"\n);\n')),(0,r.yg)("p",null,"If the Hadoop cluster has high availability and Kerberos authentication enabled, refer to the following SQL statement:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE db1.tbl1 \nPARTITION (p1, p2)\nTO "hdfs://HDFS8000871/path/to/export/" \nPROPERTIES\n(\n    "label" = "mylabel",\n    "column_separator" = ",",\n    "max_file_size" = "512MB",\n    "columns" = "col1, col2"\n)\nWITH HDFS (\n    "fs.defaultFS" = "hdfs://hacluster/",\n    "hadoop.username" = "hadoop",\n    "dfs.nameservices" = "hacluster",\n    "dfs.ha.namenodes.hacluster" = "n1, n2",\n    "dfs.namenode.rpc-address.hacluster.n1" = "192.168.0.1:8020",\n    "dfs.namenode.rpc-address.hacluster.n2" = "192.168.0.2:8020",\n    "dfs.client.failover.proxy.provider.hacluster" = "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",\n    "dfs.namenode.kerberos.principal" = "hadoop/_HOST@REALM.COM",\n    "hadoop.security.authentication" = "kerberos",\n    "hadoop.kerberos.principal" = "doris_test@REALM.COM",\n    "hadoop.kerberos.keytab" = "/path/to/doris_test.keytab"\n);\n')),(0,r.yg)("h3",{id:"export-to-s3"},"Export to S3"),(0,r.yg)("p",null,"Export all data from the ",(0,r.yg)("inlineCode",{parentName:"p"},"s3_test")," table to S3 in CSV format, using the invisible character ",(0,r.yg)("inlineCode",{parentName:"p"},"\\\\x07")," as the line delimiter."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE s3_test TO "s3://bucket/a/b/c" \nPROPERTIES (\n    "line_delimiter" = "\\\\x07"\n) WITH s3 (\n    "s3.endpoint" = "xxxxx",\n    "s3.region" = "xxxxx",\n    "s3.secret_key" = "xxxx",\n    "s3.access_key" = "xxxxx"\n);\n')),(0,r.yg)("h3",{id:"export-to-local-file-system"},"Export to Local File System"),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"To export data to the local file system, add ",(0,r.yg)("inlineCode",{parentName:"p"},"enable_outfile_to_local=true")," in ",(0,r.yg)("inlineCode",{parentName:"p"},"fe.conf")," and restart FE.")),(0,r.yg)("p",null,"Export all data from the ",(0,r.yg)("inlineCode",{parentName:"p"},"test")," table to local storage:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'-- Parquet format\nEXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n  "columns" = "k1, k2",\n  "format" = "parquet"\n);\n\n-- ORC format\nEXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n  "columns" = "k1, k2",\n  "format" = "orc"\n);\n\n-- CSV_with_names format, using \'AA\' as the column separator and \'zz\' as the line delimiter\nEXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n  "format" = "csv_with_names",\n  "column_separator" = "AA",\n  "line_delimiter" = "zz"\n);\n\n-- CSV_with_names_and_types format\nEXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n  "format" = "csv_with_names_and_types"\n);\n')),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note:\nExporting to the local file system is not suitable for public cloud users and is only applicable to private deployments. It is assumed that the user has full control over the cluster nodes. Doris does not perform legality checks on the export path. If the Doris process user does not have write permissions for the path, or if the path does not exist, an error will be reported. Additionally, for security reasons, if a file with the same name already exists in the path, the export will fail.\nDoris does not manage files exported to the local file system, nor does it check disk space. Users must manage these files themselves, such as by cleaning up as needed.")),(0,r.yg)("h3",{id:"exporting-specific-partitions"},"Exporting Specific Partitions"),(0,r.yg)("p",null,"Export tasks support exporting only specific partitions of internal tables in Doris, such as exporting only the ",(0,r.yg)("inlineCode",{parentName:"p"},"p1")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"p2")," partitions of the ",(0,r.yg)("inlineCode",{parentName:"p"},"test")," table."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test\nPARTITION (p1, p2)\nTO "file:///home/user/tmp/" \nPROPERTIES (\n    "columns" = "k1, k2"\n);\n')),(0,r.yg)("h3",{id:"filtering-data-during-export"},"Filtering Data During Export"),(0,r.yg)("p",null,"Export tasks support filtering data based on predicate conditions, exporting only data that meets the conditions, such as exporting only data where ",(0,r.yg)("inlineCode",{parentName:"p"},"k1 < 50"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test\nWHERE k1 < 50\nTO "file:///home/user/tmp/"\nPROPERTIES (\n    "columns" = "k1, k2",\n    "column_separator" = ","\n);\n')),(0,r.yg)("h3",{id:"exporting-external-table-data"},"Exporting External Table Data"),(0,r.yg)("p",null,"Export tasks support exporting external table data from the Doris Catalog:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'-- Create a catalog\nCREATE CATALOG `tpch` PROPERTIES (\n    "type" = "trino-connector",\n    "trino.connector.name" = "tpch",\n    "trino.tpch.column-naming" = "STANDARD",\n    "trino.tpch.splits-per-node" = "32"\n);\n\n-- Export data from the Catalog external table\nEXPORT TABLE tpch.sf1.lineitem TO "file:///path/to/exp_"\nPROPERTIES(\n    "parallelism" = "5",\n    "format" = "csv",\n    "max_file_size" = "1024MB"\n);\n')),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"Currently, exporting data from Catalog external tables does not support concurrent export. Even if ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," is set to greater than 1, the export is still performed in a single thread.")),(0,r.yg)("h2",{id:"best-practices"},"Best Practices"),(0,r.yg)("h3",{id:"consistent-export"},"Consistent Export"),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"Export")," function supports partition/tablet-level granularity. The ",(0,r.yg)("inlineCode",{parentName:"p"},"data_consistency")," parameter specifies the granularity for splitting the table to be exported: ",(0,r.yg)("inlineCode",{parentName:"p"},"none")," represents the tablet level, and ",(0,r.yg)("inlineCode",{parentName:"p"},"partition")," represents the partition level."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test TO "file:///home/user/tmp"\nPROPERTIES (\n    "format" = "parquet",\n    "data_consistency" = "partition",\n    "max_file_size" = "512MB"\n);\n')),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Setting ",(0,r.yg)("inlineCode",{parentName:"li"},'"data_consistency" = "partition"')," constructs multiple ",(0,r.yg)("inlineCode",{parentName:"li"},"SELECT INTO OUTFILE")," statements to export different partitions."),(0,r.yg)("li",{parentName:"ul"},"Setting ",(0,r.yg)("inlineCode",{parentName:"li"},'"data_consistency" = "none"')," constructs multiple ",(0,r.yg)("inlineCode",{parentName:"li"},"SELECT INTO OUTFILE")," statements to export different tablets, which may belong to the same partition.")),(0,r.yg)("p",null,"Refer to the appendix for the logic behind constructing ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," statements."),(0,r.yg)("h3",{id:"export-job-concurrency"},"Export Job Concurrency"),(0,r.yg)("p",null,"Set different levels of concurrency to export data concurrently. Specify a concurrency of 5:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n  "format" = "parquet",\n  "max_file_size" = "512MB",\n  "parallelism" = "5"\n);\n')),(0,r.yg)("p",null,"Refer to the appendix for the principles of concurrent export."),(0,r.yg)("h3",{id:"clear-export-directory-before-exporting"},"Clear Export Directory Before Exporting"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test TO "file:///home/user/tmp"\nPROPERTIES (\n    "format" = "parquet",\n    "max_file_size" = "512MB",\n    "delete_existing_files" = "true"\n);\n')),(0,r.yg)("p",null,"If ",(0,r.yg)("inlineCode",{parentName:"p"},'"delete_existing_files" = "true"')," is set, the export job will first delete all files and directories under ",(0,r.yg)("inlineCode",{parentName:"p"},"/home/user/"),", then export data to that directory."),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"Note:\nTo use the ",(0,r.yg)("inlineCode",{parentName:"p"},"delete_existing_files")," parameter, add ",(0,r.yg)("inlineCode",{parentName:"p"},"enable_delete_existing_files = true")," in ",(0,r.yg)("inlineCode",{parentName:"p"},"fe.conf")," and restart FE. The ",(0,r.yg)("inlineCode",{parentName:"p"},"delete_existing_files")," parameter is a dangerous operation and is recommended only for testing environments.")),(0,r.yg)("h3",{id:"set-export-file-size"},"Set Export File Size"),(0,r.yg)("p",null,"Export jobs support setting the size of export files. If a single file exceeds the set value, it will be split into multiple files."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-sql"},'EXPORT TABLE test TO "file:///home/user/tmp/"\nPROPERTIES (\n    "format" = "parquet",\n    "max_file_size" = "512MB"\n);\n')),(0,r.yg)("p",null,"Setting ",(0,r.yg)("inlineCode",{parentName:"p"},'"max_file_size" = "512MB"')," limits the maximum size of a single export file to 512MB."),(0,r.yg)("h2",{id:"notes"},"Notes"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Memory Limits"),(0,r.yg)("p",{parentName:"li"},"  An Export job typically involves only ",(0,r.yg)("inlineCode",{parentName:"p"},"scan-export")," operations and does not require complex memory-consuming computations. The default 2GB memory limit usually suffices."),(0,r.yg)("p",{parentName:"li"},"  In scenarios where the query plan needs to scan too many tablets or versions on the same BE, memory might run out. Adjust the ",(0,r.yg)("inlineCode",{parentName:"p"},"exec_mem_limit")," session variable to increase the memory limit.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Export Data Volume"),(0,r.yg)("p",{parentName:"li"},"  Avoid exporting a large volume of data at once. The recommended maximum data volume for an Export job is several tens of GBs. Larger exports can lead to more garbage files and higher retry costs. If the table size is too large, consider partition-based export."),(0,r.yg)("p",{parentName:"li"},"  Export jobs scan data and occupy IO resources, which may affect system query latency.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Managing Export Files"),(0,r.yg)("p",{parentName:"li"},"  If an Export job fails, the generated files are not deleted and need to be removed manually.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Data Consistency"),(0,r.yg)("p",{parentName:"li"},"  During export, the system simply checks if the tablet versions are consistent. It is advisable to avoid importing data into the table during the export process.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Export Timeout"),(0,r.yg)("p",{parentName:"li"},"  If the data volume is large and exceeds the export timeout, the Export job will fail. Use the ",(0,r.yg)("inlineCode",{parentName:"p"},"timeout")," parameter in the Export command to extend the timeout and retry the Export command.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Export Failure"),(0,r.yg)("p",{parentName:"li"},"  If the FE restarts or switches primary during the Export job, the job will fail and need to be resubmitted. Use the ",(0,r.yg)("inlineCode",{parentName:"p"},"show export")," command to check the Export job status.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Number of Partitions Exported"),(0,r.yg)("p",{parentName:"li"},"  The maximum number of partitions an Export Job can export is 2000. Modify this limit by adding the ",(0,r.yg)("inlineCode",{parentName:"p"},"maximum_number_of_export_partitions")," parameter in ",(0,r.yg)("inlineCode",{parentName:"p"},"fe.conf")," and restarting FE.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Concurrent Export"),(0,r.yg)("p",{parentName:"li"},"  When exporting concurrently, configure the thread count and parallelism appropriately to fully utilize system resources and avoid performance bottlenecks. Monitor the progress and performance metrics in real-time to identify and address issues promptly.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Data Integrity"),(0,r.yg)("p",{parentName:"li"},"  After the export operation is complete, verify that the exported data is complete and correct to ensure data quality and integrity."))),(0,r.yg)("h2",{id:"appendix"},"Appendix"),(0,r.yg)("h3",{id:"principles-of-concurrent-export"},"Principles of Concurrent Export"),(0,r.yg)("p",null,"The underlying mechanism of an Export task in Doris is to execute ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," SQL statements. When a user initiates an Export task, Doris constructs one or more ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," execution plans based on the table to be exported. These execution plans are then submitted to Doris's Job Scheduler, which automatically schedules and executes them."),(0,r.yg)("p",null,"By default, Export tasks run single-threaded. To improve export efficiency, the Export command can include a ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," parameter to enable concurrent data export. Setting ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," greater than 1 allows the Export task to use multiple threads to concurrently execute ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," query plans. The ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," parameter specifies the number of threads to execute the EXPORT job."),(0,r.yg)("p",null,"The logic for constructing one or more ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," execution plans for an Export task is as follows:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Select Consistency Model"),"\nChoose the consistency model for export based on the ",(0,r.yg)("inlineCode",{parentName:"p"},"data_consistency")," parameter. This is semantic and unrelated to concurrency. Users should select the consistency model according to their needs.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Determine Concurrency"),"\nThe ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," parameter determines the number of threads to run the ",(0,r.yg)("inlineCode",{parentName:"p"},"SELECT INTO OUTFILE")," execution plans. ",(0,r.yg)("inlineCode",{parentName:"p"},"Parallelism")," specifies the maximum possible number of threads."),(0,r.yg)("blockquote",{parentName:"li"},(0,r.yg)("p",{parentName:"blockquote"},"Note: Even if the Export command sets the ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," parameter, the actual number of concurrent threads for the Export task also depends on Job Scheduler resources. If the system is busy and Job Scheduler thread resources are tight, the actual number of threads assigned to the Export task might not reach the specified ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism"),", affecting the concurrent export. To address this, reduce system load or adjust the FE configuration ",(0,r.yg)("inlineCode",{parentName:"p"},"async_task_consumer_thread_num")," to increase the total number of Job Scheduler threads."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Determine Task Volume for Each Outfile Statement"),"\nEach thread decides the number of ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," based on ",(0,r.yg)("inlineCode",{parentName:"p"},"maximum_tablets_of_outfile_in_export")," and the actual number of partitions/buckets in the data."),(0,r.yg)("blockquote",{parentName:"li"},(0,r.yg)("p",{parentName:"blockquote"},(0,r.yg)("inlineCode",{parentName:"p"},"maximum_tablets_of_outfile_in_export")," is an FE configuration with a default value of 10. It specifies the maximum number of partitions/buckets allowed in a single ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," statement for an Export task. Modifying this configuration requires restarting FE.")))),(0,r.yg)("h3",{id:"example"},"Example"),(0,r.yg)("p",null,"Consider a table with 20 partitions, each having 5 buckets, totaling 100 buckets. Set ",(0,r.yg)("inlineCode",{parentName:"p"},"data_consistency = none")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"maximum_tablets_of_outfile_in_export = 10"),"."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"parallelism = 5")),"\nThe Export task splits the 100 buckets into 5 parts, with each thread responsible for 20 buckets. Each thread further splits its 20 buckets into 2 groups of 10 buckets each, with each group handled by one ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," query plan. Thus, the Export task has 5 threads running concurrently, each handling 2 ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," statements, which are executed serially within each thread.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"parallelism = 3")),"\nThe Export task splits the 100 buckets into 3 parts, with 3 threads responsible for 34, 33, and 33 buckets respectively. Each thread splits its buckets into 4 groups of 10 buckets (with the last group containing fewer than 10 buckets), each group handled by one ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," query plan. Thus, the Export task has 3 threads running concurrently, each handling 4 ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," statements, executed serially within each thread.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},(0,r.yg)("inlineCode",{parentName:"strong"},"parallelism = 120")),"\nSince the table has only 100 buckets, the system forces ",(0,r.yg)("inlineCode",{parentName:"p"},"parallelism")," to 100 and executes it accordingly. The Export task splits the 100 buckets into 100 parts, with each thread responsible for 1 bucket. Each thread's single bucket is split into 1 group (actually just 1 bucket), handled by one ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," query plan. Thus, the Export task has 100 threads running concurrently, each handling 1 ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," statement, with each ",(0,r.yg)("inlineCode",{parentName:"p"},"outfile")," statement exporting just 1 bucket."))),(0,r.yg)("h3",{id:"optimizing-export-performance"},"Optimizing Export Performance"),(0,r.yg)("p",null,"For optimal Export performance in the current version, consider the following settings:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Enable the session variable ",(0,r.yg)("inlineCode",{parentName:"li"},"enable_parallel_outfile"),"."),(0,r.yg)("li",{parentName:"ol"},"Set the Export's ",(0,r.yg)("inlineCode",{parentName:"li"},"parallelism")," parameter to a high value, so each thread handles only one ",(0,r.yg)("inlineCode",{parentName:"li"},"SELECT INTO OUTFILE")," query plan."),(0,r.yg)("li",{parentName:"ol"},"Set the FE configuration ",(0,r.yg)("inlineCode",{parentName:"li"},"maximum_tablets_of_outfile_in_export")," to a low value, so each ",(0,r.yg)("inlineCode",{parentName:"li"},"SELECT INTO OUTFILE")," query plan exports a small amount of data.")))}u.isMDXComponent=!0}}]);